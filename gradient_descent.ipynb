{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c461c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.neural_net import Neuron, Layer, MLP\n",
    "from micrograd.viz import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb5fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of 4 items\n",
    "xs = [\n",
    "    [2,3,-1],\n",
    "    [3,-1,0.5],\n",
    "    [0.5,1,1],\n",
    "    [1,1,-1]\n",
    "]\n",
    "ys = [1, -1, -1, 1]\n",
    "\n",
    "step_size = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ca2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptually, this is a function that takes in an x_i and returns a scalar (well, 1d vector)\n",
    "# Representation wise, right now it just has a bunch of disconnected Value objects. They're not linked yet\n",
    "model = MLP(3, [4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aedf4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first output = -0.7105225964370366\n",
      "after one step = -0.727294720296473\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent on f(w) -> MLP output.\n",
    "# This is not what we should be doing -- we want to decrease the loss function, not the output\n",
    "# on 1 data point, but it's just to show that we can gradient descent on anything.\n",
    "# Invoking MLP on an input creates the network, and performs a forward pass.\n",
    "val = model(xs[0])[0]\n",
    "print(f\"first output = {val.data}\")\n",
    "val.run_full_backpropagation()\n",
    "for param in model.parameters:\n",
    "    param.data -= param.grad * step_size\n",
    "\n",
    "val = model(xs[0])[0]\n",
    "print(f\"after one step = {val.data}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c8dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first loss = 2.983547050764071\n",
      "after one step = 2.7853898529896064\n"
     ]
    }
   ],
   "source": [
    "# Gradient desccent to reduce f(w) = loss on the first data point\n",
    "loss = (model(xs[0])[0] - ys[0]) ** 2\n",
    "print(f\"first loss = {loss.data}\")\n",
    "loss.run_full_backpropagation()\n",
    "\n",
    "for param in model.parameters:\n",
    "    param.data -= param.grad * step_size\n",
    "\n",
    "loss = (model(xs[0])[0] - ys[0]) ** 2\n",
    "print(f\"after one step = {loss.data}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fdc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 loss = 8.752951887561764\n",
      "Step 1 loss = 3.704948474777731\n",
      "Step 2 loss = 2.9660303981129092\n",
      "Step 3 loss = 4.069500881426543\n",
      "Step 4 loss = 0.5144979684255687\n",
      "Step 5 loss = 0.2319877389750086\n",
      "Step 6 loss = 0.13691909258194718\n",
      "Step 7 loss = 0.09893503695276235\n",
      "Step 8 loss = 0.07740768501491944\n",
      "Step 9 loss = 0.06347911048000197\n",
      "Step 10 loss = 0.053746571752151695\n",
      "Step 11 loss = 0.046570558678917365\n",
      "Step 12 loss = 0.041065099718451153\n",
      "Step 13 loss = 0.03671018182350557\n",
      "Step 14 loss = 0.033180798401665254\n",
      "Step 15 loss = 0.03026352354651262\n",
      "Step 16 loss = 0.027812488811025108\n",
      "Step 17 loss = 0.025724654529611573\n",
      "Step 18 loss = 0.023925190040531667\n",
      "Step 19 loss = 0.022358453241665657\n",
      "Step 20 loss = 0.02098221973067667\n",
      "Step 21 loss = 0.019763874238788077\n",
      "Step 22 loss = 0.018677828276136562\n",
      "Step 23 loss = 0.017703727216988176\n",
      "Step 24 loss = 0.016825179135218107\n",
      "Step 25 loss = 0.01602883658172381\n",
      "Step 26 loss = 0.015303722119340932\n",
      "Step 27 loss = 0.014640725376161134\n",
      "Step 28 loss = 0.014032222836891078\n",
      "Step 29 loss = 0.013471786818834718\n",
      "Step 30 loss = 0.012953960162061497\n",
      "Step 31 loss = 0.012474079962553578\n",
      "Step 32 loss = 0.012028138338907916\n",
      "Step 33 loss = 0.011612671468620845\n",
      "Step 34 loss = 0.01122467042143398\n",
      "Step 35 loss = 0.01086150895628063\n",
      "Step 36 loss = 0.010520884635021129\n",
      "Step 37 loss = 0.010200770474958185\n",
      "Step 38 loss = 0.009899375004908355\n",
      "Step 39 loss = 0.009615109069847399\n",
      "Step 40 loss = 0.009346558091246335\n",
      "Step 41 loss = 0.009092458765599932\n",
      "Step 42 loss = 0.008851679394781554\n",
      "Step 43 loss = 0.00862320320497337\n",
      "Step 44 loss = 0.008406114137843906\n",
      "Step 45 loss = 0.00819958469708712\n",
      "Step 46 loss = 0.008002865511845147\n",
      "Step 47 loss = 0.007815276340745147\n",
      "Step 48 loss = 0.007636198289918945\n",
      "Step 49 loss = 0.007465067058204343\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50\n",
    "step_size = 0.1\n",
    "for step in range(num_steps):\n",
    "    # Conceptually, loss is a function that takes in weights, and returns a loss scalar.\n",
    "    # This creates the network and perform a forward pass on the entire dataset\n",
    "    loss = sum([(model(xs[i])[0] - ys[i])** 2 for i in range(len(ys))])\n",
    "\n",
    "    # Run backprop to compute dloss(w)/dw for all w params.\n",
    "    loss.run_full_backpropagation()\n",
    "    \n",
    "    # Take the opposite gradient step\n",
    "    for param in model.parameters:\n",
    "        param.data -= step_size * param.grad\n",
    "    \n",
    "    print(f\"Step {step} loss = {loss.data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51604f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9725684253675214, -0.9668185724655831, -0.9449641292667839, 0.9508174668459224]\n",
      "[1, -1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "print([model(x)[0].data for x in xs])\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ed62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
